# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: vctk16k.yaml
  - override /model: vqautoencoder.yaml
  - override /callbacks: custom_exp2.yaml
  - override /trainer: gpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["vctk16k", "vqautoencoder"]
codebook_dim : 512
codebook_size: 1024
seed: 12345

trainer:
  min_epochs: 30
  max_epochs: 30

datamodule:
  batch_size: 12
  audio_len: 32000
  
model:
  apply_grad_penalty: False
  stft_normalized: True
  warmup_steps: 0
  autoencoder:
    _target_: src.models.components.backbones.autoencoder1d.VQAutoEncoder1d
    in_channels: 1
    channels: 96
    num_filters: 96
    resnet_groups: 8
    stride: 1
    window_length: 3
    use_nearest_upsample: True
    multipliers: [1, 4, 4, 4, 4, 8, 8]
    factors: [2, 2, 2, 2, 2, 2]
    num_blocks: [2, 2, 2, 2, 2, 2]
    codebook_dim: ${codebook_dim}
    
  bottleneck:
    _target_: src.models.components.backbones.bottleneck.VQBottleneck
    dim: ${codebook_dim}
    codebook_size: ${codebook_size}
  
  vqstftdiscriminator:
    _target_: src.models.components.backbones.rvqautoencoder.ComplexSTFTDiscriminator
    channels: 16
    n_fft: 1024
    hop_length: 256
    win_length: 1024
    stft_normalized: True

logger:
  wandb:
    tags: ${tags}
    group: "vq-ae"
